# elevata â€“ Airflow example
# Optional orchestration setup using Apache Airflow.
#
# This file is provided as an example and is not required for elevata usage.


x-airflow-common: &airflow-common
  build:
    context: ../..
    dockerfile: examples/airflow/Dockerfile
    
  env_file:
    - ../../.env

  environment:
    TZ: ${TZ:-UTC}

    # --- Airflow 3 API / auth (required for scheduler<->api-server handshake) ---
    AIRFLOW__API__BASE_URL: "http://localhost:8080"
    # Airflow 3.x: execution API server URL is in [core], not [api]
    # MUST include /execution/ and should be reachable from scheduler container
    AIRFLOW__CORE__EXECUTION_API_SERVER_URL: "http://airflow-webserver:8080/execution/"

    AIRFLOW__WEBSERVER__BASE_URL: "http://localhost:8080"
    # Airflow 3: secret_key moved from [webserver] -> [api]; must be identical across ALL components
    AIRFLOW__API__SECRET_KEY: "demo-elevata-airflow-secret-key-change-me-please-64chars-min-xxxxxxxxxxxxxxxxxxxxxxxx"
    # Some Airflow 3.x builds still read secret_key from [core] for signing internal JWTs
    AIRFLOW__CORE__SECRET_KEY: "demo-elevata-airflow-secret-key-change-me-please-64chars-min-xxxxxxxxxxxxxxxxxxxxxxxx"
    # Optional backward-compat (harmless)
    AIRFLOW__WEBSERVER__SECRET_KEY: "demo-elevata-airflow-secret-key-change-me-please-64chars-min-xxxxxxxxxxxxxxxxxxxxxxxx"
    # JWT key for scheduler <-> execution API auth (Airflow 3.x)
    AIRFLOW__API_AUTH__JWT_SECRET: "demo-elevata-airflow-jwt-secret-key-change-me-please-64bytes-min-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
    # optional (default when jwt_secret is set is HS512)
    AIRFLOW__API_AUTH__JWT_ALGORITHM: "HS512"
    # Optional but good for consistency across containers (encryption of connections/vars).
    AIRFLOW__CORE__FERNET_KEY: "QW0yNfF8m9jJQH5tqJ4e1xqHhUu8d8o6s2mQY8N3i4Q="

    # Served logs (fallback path): without host Airflow builds http://:8793/... (invalid)
    # With LocalExecutor tasks run on the scheduler container.
    AIRFLOW__LOGGING__WORKER_LOG_SERVER_HOST: airflow-scheduler
    AIRFLOW__LOGGING__WORKER_LOG_SERVER_PORT: "8793"
    AIRFLOW__LOGGING__WORKER_LOG_SERVER_BIND_HOST: "0.0.0.0"
    # Airflow 3.x: "file.task" deprecated; use "task"
    AIRFLOW__LOGGING__TASK_LOG_READER: "task"
    AIRFLOW__LOGGING__BASE_LOG_FOLDER: /opt/airflow/logs

    # --- Airflow core ---
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "true"
    AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
    AIRFLOW__CORE__DEFAULT_TIMEZONE: Europe/Berlin
    AIRFLOW__WEBSERVER__DEFAULT_UI_TIMEZONE: Europe/Berlin

    # --- Admin user for FAB (created by airflow-init) ---
    AIRFLOW_ADMIN_USER: ${AIRFLOW_ADMIN_USER:-admin}
    AIRFLOW_ADMIN_PASSWORD: ${AIRFLOW_ADMIN_PASSWORD:-admin}
    AIRFLOW_ADMIN_EMAIL: ${AIRFLOW_ADMIN_EMAIL:-admin@example.com}

    # --- Optional: set a predictable DAG folder ---
    AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags

    # --- elevata defaults used by the example DAG ---
    # elevata runtime config comes from env_file (../../.env).
    # Defaults are applied in entrypoint.sh if variables are missing.

    # Optional: staleness warning threshold
    ELEVATA_MANIFEST_MAX_AGE_HOURS: ${ELEVATA_MANIFEST_MAX_AGE_HOURS:-24}

  volumes:
    # Mount DAGs and elevata repo
    # NOTE: these variables must be available at *compose parse time*
    # (Compose reads them from ./examples/airflow/.env or from the shell env).
    # `env_file:` does NOT apply to variable substitution in this section.
    - ${ELEVATA_AIRFLOW_DAGS_DIR:-./dags}:/opt/airflow/dags:ro
    - ${ELEVATA_REPO_DIR:-../..}:/opt/elevata:rw

    # Shared Airflow logs (so the UI can read logs without served-log URLs)
    - airflow_logs:/opt/airflow/logs:rw

  working_dir: /opt/elevata/core
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on:
    postgres:
      condition: service_healthy

services:
  postgres:
    image: postgres:16
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 10
    volumes:
      - airflow_pgdata:/var/lib/postgresql/data

  airflow-init:
    <<: *airflow-common
    user: "0:0"
    hostname: airflow-init
    entrypoint: /bin/bash
    command:
      - -c
      - |
        set -euo pipefail
        chown -R ${AIRFLOW_UID:-50000}:0 /opt/airflow/logs
        chmod -R g+rwX /opt/airflow/logs
        su airflow -c "airflow db migrate"
        su airflow -c "airflow users create \
          --username admin \
          --password admin \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com" || true
    restart: "no"

  airflow-webserver:
    <<: *airflow-common
    hostname: airflow-webserver
    command:
      - airflow
      - api-server
      - --host
      - 0.0.0.0
      - --port
      - "8080"
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "bash", "-lc", "python - << 'PY'\nimport requests\nr=requests.get('http://localhost:8080/api/v2/monitor/health', timeout=3)\nprint(r.status_code)\nraise SystemExit(0 if r.status_code==200 else 1)\nPY"]
      interval: 5s
      retries: 20
      start_period: 10s
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  airflow-dag-processor:
    <<: *airflow-common
    hostname: airflow-dag-processor
    command:
      - airflow
      - dag-processor
    depends_on:
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    hostname: airflow-scheduler
    command:
      - airflow
      - scheduler
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      airflow-webserver:
        condition: service_healthy

  airflow-triggerer:
    <<: *airflow-common
    hostname: airflow-triggerer
    command:
      - airflow
      - triggerer
    depends_on:
      airflow-init:
        condition: service_completed_successfully

volumes:
  airflow_pgdata:
  airflow_logs:

# âš™ï¸ Load SQL Architecture

This document describes how elevata transforms metadata into executable SQL for load operations.

---

## ðŸ”§ 1. Overview

The load SQL pipeline turns metadata into **complete SQL statements** suitable for execution on various analytical backends.  

Highâ€‘level flow:

```
Metadata â†’ Logical Plan â†’ Expression AST â†’ Dialect Rendering â†’ SQL Load Statement
```

The system is designed so that:  
- Metadata stays backendâ€‘agnostic  
- Logical Plans describe *what* is needed, not *how* it is written  
- Dialects control syntactic differences  
- Execution engines (DuckDB, Postgres, MSSQL, ...) remain fully decoupled

---

## ðŸ”§ 2. Logical Plans for Load Operations

Elevata represents load operations using SQL primitives:  

- `LogicalSelect` â€“ core building block  
- `LogicalUnion` â€“ multi-source resolution  
- `SubquerySource` â€“ ranking, filtering, pre-aggregation  

The Logical Plan is intentionally **dialect-neutral**.

---

## ðŸ”§ 3. Expression AST in Load SQL

During load generation, every column expression is represented as an AST node.  

Supported nodes include:  
- `ColumnRef`  
- `Literal`  
- `ExprRef`  
- `ConcatExpr`  
- `ConcatWsExpr`  
- `CoalesceExpr`  
- `WindowFunctionExpr`  
- `Hash256Expr`  

The AST guarantees that:  
- hashing is consistent across dialects  
- CONCAT/COALESCE behave uniformly  
- window functions are structured, not stringâ€‘built  
- SQL remains predictable and comparable

---

## ðŸ”§ 4. Dialect Rendering

After the Logical Plan is constructed, the load generator asks the selected dialect to turn AST + plan into SQL.

```
sql = dialect.render_select(plan)
```

Each dialect implements:  
- identifier quoting (Postgres â†’ "col", MSSQL â†’ "col", DuckDB â†’ "col")  
- literal rendering (strings, numbers, nulls)  
- hashing functions (SHAâ€‘256 across all dialects)  
- CONCAT / CONCAT_WS  
- COALESCE  
- window functions (ROW_NUMBER)  
- subqueries and unions  

This ensures consistent behavior while using native SQL syntax per backend.

---

## ðŸ”§ 5. Load Runner

The **Load Runner CLI** (`elevata_load`) executes the SQL generated by the Logical Plan + dialect.  

Features:  
- resolves active profile  
- reads target dataset metadata  
- constructs Logical Plan  
- generates SQL via dialect  
- executes SQL or performs a dryâ€‘run  

Example:
```bash
python manage.py elevata_load customers_rawcore --dry-run
```

The Load Runner and SQL Preview use the **same generation pipeline**.

---

## ðŸ”§ 6. Deterministic Generation

The load SQL generator is fully deterministic:  
- stable BK ordering  
- stable hashing patterns  
- stable naming (`__src_rank_ord`)  
- stable Logical Plan tree  

This guarantees reproducible SQL, predictable diffs, and reliable migration workflows.

---

## ðŸ”§ 7. Summary

Elevataâ€™s load SQL architecture is built on three principles:  

1. **Metadata-driven** â€“ business modeling defines everything  
2. **Dialect-neutral logical plans** â€“ backend independence by design  
3. **AST-based SQL generation** â€“ correctness, portability, and composability  

This ensures that SQL loads are stable, transparent, and backendâ€‘portable, while leaving room for extended merge/delete features.

---

## ðŸ”§ 8. Load observability & debugging

The load SQL pipeline not only generates SQL, it also exposes lightweight  
observability hooks that make load runs traceable and debuggable across  
profiles, target systems, and dialects.

### ðŸ§© 8.1 Load run summary

Before generating load SQL, the system builds a small summary for each target
dataset:  

- `schema` â€“ logical schema short name (e.g. `rawcore`)  
- `dataset` â€“ target dataset name (e.g. `rc_aw_sales_order`)  
- `mode` â€“ load mode chosen by the planner (`full`, `append`, `merge`, `snapshot`, â€¦)  
- `handle_deletes` â€“ whether delete detection is active for this run  
- `historize` â€“ whether the dataset participates in historization  
- `dialect` â€“ active SQL dialect name (e.g. `duckdb`, `postgres`, `mssql`)  

This summary is created by helper functions in the load SQL layer  
(`build_load_run_summary(...)`, `format_load_run_summary(...)`) and is used  
by the `elevata_load` command for logging and debug output.

### ðŸ§© 8.2 Batch and load run IDs

Each invocation of `elevata_load` generates a **batch run ID** that acts as  
a logical wrapper for all datasets processed in that command:  

- `batch_run_id` â€“ one per command invocation (even if only one dataset is loaded)  
- `load_run_id` â€“ one per dataset within the batch  

This allows grouping all per-dataset runs that belong to the same batch  
(e.g. a nightly job that refreshes multiple RAWCORE targets).

### ðŸ§© 8.3 CLI logging

The `elevata_load` management command logs both start and end of each dataset  
load using the load run summary and IDs above. Logged attributes include:  

- `batch_run_id`, `load_run_id`  
- target dataset ID and name  
- target schema, profile, target system and dialect  
- chosen load mode (`mode`, `handle_deletes`, `historize`)  
- SQL length and render time  
- start/end timestamps (for duration calculations)  

In dry-run mode, the command prints the load run summary alongside the
generated SQL, for example:

```text
-- Load summary: [duckdb] rawcore.rc_aw_sales_order mode=merge, deletes=True, historize=False
```

This makes it easy to understand how a dataset would be loaded before any
execution layer is wired up.

### ðŸ§© 8.4 Future: warehouse-level load run log

The current architecture is designed so that the same summary data can later  
be written into a warehouse-side load_run_log table (per target system).  
Such a table would be used for reporting use cases like:  

- "When was this dataset last successfully loaded?"  
- "Which datasets were part of a given batch run?"  

DDL and execution for this warehouse-level log are intentionally deferred  
until the execution layer (execute=True in elevata_load) is fully wired.

> **Next planned step**
>
> Warehouse-level logging for reporting:
> - insert per load_run into `<meta>.load_run_log`
> - track success/error & rows affected
> - show â€œdata freshnessâ€ in BI

---

## ðŸ”§ 9. CLI usage examples

The `elevata_load` management command supports different usage scenarios â€”  
from printing SQL for a single target dataset to executing multiple loads in a batch.

> These examples use read-only / dry-run mode by default.
> Execution (`--execute`) will be enabled later in the roadmap.

### ðŸ§© 9.1 Preview generated SQL for one target dataset

```bash
python manage.py elevata_load rc_aw_sales_order --schema rawcore
```

This prints a load summary and the generated SQL to stdout.

### ðŸ§© 9.2 Preview SQL for a specific dialect and target system

```bash
python manage.py elevata_load rc_aw_sales_order --schema rawcore --dialect duckdb --target-system local_duckdb
```

Useful to verify dialect differences (SQL syntax, merge support, â€¦).

### ðŸ§© 9.3 Debug end-to-end LoadPlan (no execution)

```bash
python manage.py elevata_load rc_customer --schema rawcore --debug-plan
```

Outputs:  

- Load mode (`merge`, `append`, â€¦)  
- Delete-detection / historization flags  
- Logical plan (`repr`)  
- SELECT preview  
- Load SQL  

Helpful while developing or supporting new datasets.

### ðŸ§© 9.4 Generate SQL silently (no console output)

```bash
python manage.py elevata_load rc_supplier --schema rawcore --no-print
```

Useful for automation where output is handled by logging only.

### ðŸ§© 9.5 Logging to structured JSON (application logs only)

```bash
python manage.py elevata_load rc_product --schema rawcore --log-json
```

Downstream log shipping / ingestion tools can parse the JSON metadata.

### ðŸ§© 9.6 Run by schema (load multiple datasets at once)

```bash
python manage.py elevata_load --schema rawcore
```

All historized `rawcore` targets will be planned and logged under one
**batch_run_id**, allowing full-batch reporting later.

### ðŸ§© 9.7 Execute SQL in the target system (*future*)

```bash
python manage.py elevata_load rc_customer --schema rawcore --execute
```

---

#### ðŸ”Ž Tip: combine flags

```bash
python manage.py elevata_load rc_aw_product --schema rawcore --debug-plan --log-json
```

Perfect for troubleshooting CI/CD pipelines.

### ðŸ§© Summary Table

| Scenario | Command | Output |
|---------|---------|--------|
| Single dataset preview | `dataset_name` | SQL printed |
| Profile/system override | `--target-system ...` | SQL adapts to environment |
| Plan inspection | `--debug-plan` | logical plan + SQL |
| Quiet automation | `--no-print` | log only |
| Log ingestion | `--log-json` | structured JSON |
| Batch support | `--schema ...` | multiple load_run_ids in one batch_run_id |
| Execution | `--execute` *(soon)* | executes in warehouse |

---

## ðŸ”§10. Execute mode (`--execute`)

The `elevata_load` management command exposes a flag:

```bash
python manage.py elevata_load <target_name> --schema rawcore --execute
```

Intern this will:  
- resolve the active SQL dialect and target system,  
- ask the dialect for an execution engine (dialect.get_execution_engine(system)),
- call engine.execute(sql) for the rendered load statement.  

However, at the moment no dialect provides a fully wired execution engine yet.  
The current behaviour is:
- --execute is accepted by the CLI,  
- the dialect returns a stub execution engine,  
- the engine raises NotImplementedError,  
- elevata_load converts this into a CommandError with a clear message,  
e.g. "DuckDB execution is not wired yet for system 'dwh'."  

This keeps the execution contract and CLI stable, while postponing the actual  
warehouse integration (connections, transactions, load-run logging) to a later version.

---

Â© 2025 elevata Labs â€” Internal Technical Documentation
